{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EEG Stroke - BCI MI\n",
    "\n",
    "## Processing pipeline of the EEG data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction & methodology\n",
    "\n",
    "This notebook depicts the exploration of a pipeline for processing EEG data from elbow movement (extension/flexion) and perform a classification task. It is part of a series of notebooks developed with the objective to find a standard pipeline for EEG elbow movement classification.\n",
    "\n",
    "This notebook specifically tackles the implementation of a spatial filtering based method, using CSP or xDAWN spatial filters. The goal is to establish the performance of the implemented pipeline. To that end, we present the steps that we will follow:\n",
    "\n",
    "1. Preprocessing:\n",
    "    * Load the data from each `.npy` file, using the `DataLoader` class.\n",
    "    * Only use the recorded data of the arm opposite to the stroke side.\n",
    "    * Only use the electrodes from the stroke side.\n",
    "    * Label the raw data using the acceleration records (check `DataLoader` implementation).\n",
    "    * Filter the data between 4Hz and 40Hz.\n",
    "    * Epoch data around the movement onset (1=extension, 2=flexion, 0=remaining). *Note: data labelled 0 is taken using epoching from -4s to -1s.*\n",
    "    * Split data into 'train'/'test' sets. Get balanced sets.\n",
    "    * Label movement (1 and 2) as 1 and no onset as 0.\n",
    "2. Feature extraction:\n",
    "    * CSP (spatial filtering)\n",
    "    * xDAWN (spatial filtering) + covariance\n",
    "    * xDAWN (spatial filtering) + band power\n",
    "3. Classification: (comparison of several classifiers)\n",
    "    * Logistic Regression\n",
    "    * LDA\n",
    "    * SVM\n",
    "    * Random Forest\n",
    "4. Evaluation: Test A, B, C\n",
    "\n",
    "NB: \n",
    "Additional pipeline has been implemented and tested. As suggested by the good separation data with CSP (see part on CSP visualization), we imagined a Voting Classifier with diverse elementary pipeline [CSP + classif] trained on one session each. The test is depicted in the last section (Pipeline n°4). The results are not great, mainly due to the fact that despite good separation in the patterns space after the CSP, the distribution of the data is not centered, nor rescaled... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import DataLoader\n",
    "\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering bounds\n",
    "FILTER_FMIN = 4\n",
    "FILTER_FMAX = 40\n",
    "\n",
    "# Epoching bounds\n",
    "EPOCHS_TMIN = -1.5\n",
    "EPOCHS_TMAX = +1.5\n",
    "EPOCHS_TMIN_NO_ONSET = -5.0\n",
    "EPOCHS_TMAX_NO_ONSET = EPOCHS_TMIN_NO_ONSET + EPOCHS_TMAX - EPOCHS_TMIN\n",
    "\n",
    "# Set seed\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "BINARY_CLASSIFICATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_SESSIONS = 10\n",
    "\n",
    "FOLDER_PATH = '../../data/raw/'\n",
    "FILES_PATHS = [FOLDER_PATH + file_path for file_path in os.listdir(FOLDER_PATH) if file_path.endswith('.npy')]\n",
    "\n",
    "# Show paths\n",
    "FILES_PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channels(raws, side):\n",
    "    endings = ('1', '3', '5', '7', '9') if side=='D' else ('2', '4', '6', '8', '10')\n",
    "    channels_to_remove = [channel for channel in raws.ch_names if channel.endswith(endings)]\n",
    "    channels = [channel for channel in raws.ch_names if channel not in channels_to_remove]\n",
    "    return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(file_list, verbose=False, return_epochs=False):\n",
    "    patients_data   = []\n",
    "    patients_labels = []\n",
    "    patients_id = []\n",
    "    sessions_id = []\n",
    "    patients_epochs = []\n",
    "\n",
    "    for fid in tqdm.tqdm(file_list):\n",
    "\n",
    "        # Load the data\n",
    "        data_loader = DataLoader(fid)\n",
    "\n",
    "        # Pick the arm session opposite to the stroke side\n",
    "        stroke_side = data_loader.stroke_side\n",
    "        side = 'G' if stroke_side == 'D' else 'D'\n",
    "        raws = data_loader.get_raws(side)\n",
    "\n",
    "        # if no data for the arm side, skip\n",
    "        if raws is None:\n",
    "            continue\n",
    "\n",
    "        # Pick the channels of the stroke side\n",
    "        raws.pick_channels(get_channels(raws, stroke_side))\n",
    "\n",
    "        # Filter between 4Hz - 48Hz\n",
    "        raws.filter(FILTER_FMIN, FILTER_FMAX, fir_design='firwin')\n",
    "\n",
    "        # Epochs over flexion and extension\n",
    "        events = mne.find_events(raws, stim_channel=['movement'])\n",
    "        picks  = mne.pick_types(raws.info, eeg=True, stim=False)\n",
    "        epochs = mne.Epochs(raws, events, tmin=EPOCHS_TMIN, tmax=EPOCHS_TMAX, picks=picks, baseline=None, preload=True)\n",
    "\n",
    "        epochs_X = epochs.get_data()  # data\n",
    "        epochs_y = events[:, -1]      # labels\n",
    "\n",
    "        # Epochs of no movement artificially created\n",
    "        epochs_no_onset = mne.Epochs(raws, events, tmin=EPOCHS_TMIN_NO_ONSET, tmax=EPOCHS_TMAX_NO_ONSET, picks=picks, baseline=None, preload=True)\n",
    "\n",
    "        epochs_no_onset_X = epochs_no_onset.get_data()           # data\n",
    "        epochs_no_onset_y = np.zeros(epochs_no_onset_X.shape[0]) # labels\n",
    "\n",
    "        # Concatenate epochs\n",
    "        epochs_X_session = np.concatenate([epochs_X, epochs_no_onset_X], axis=0)\n",
    "        epochs_y_session = np.concatenate([epochs_y, epochs_no_onset_y], axis=0)\n",
    "\n",
    "        # Create epochs object\n",
    "        events_no_onset = events.copy()\n",
    "        events_no_onset[:, 2] = 0\n",
    "        events_no_onset[:, 0] = events_no_onset[:,0] - int((EPOCHS_TMAX-EPOCHS_TMIN) * raws.info['sfreq'])\n",
    "\n",
    "        events_all = np.concatenate([events, events_no_onset], axis=0)\n",
    "        epochs_all = mne.EpochsArray(\n",
    "            epochs_X_session,\n",
    "            epochs.info,\n",
    "            events=events_all,\n",
    "            tmin=epochs.tmin,\n",
    "            )\n",
    "\n",
    "        # Shuffle epochs\n",
    "        rng = np.random.RandomState(RANDOM_STATE)\n",
    "        idx = np.arange(epochs_X_session.shape[0])\n",
    "        rng.shuffle(idx)\n",
    "        epochs_X_session = epochs_X_session[idx]\n",
    "        epochs_y_session = epochs_y_session[idx]\n",
    "\n",
    "        # Merge flexion and extension if needed\n",
    "        if BINARY_CLASSIFICATION:\n",
    "            epochs_y_session[epochs_y_session > 0] = 1\n",
    "\n",
    "        # Append to the list\n",
    "        if data_loader.patient_id not in patients_id:\n",
    "            patients_id.append(data_loader.patient_id)\n",
    "            sessions_id.append([])\n",
    "            patients_data.append([])\n",
    "            patients_labels.append([])\n",
    "            patients_epochs.append([])\n",
    "        sessions_id[patients_id.index(data_loader.patient_id)].append(data_loader.session_id)\n",
    "        patients_data[patients_id.index(data_loader.patient_id)].append(epochs_X_session)\n",
    "        patients_labels[patients_id.index(data_loader.patient_id)].append(epochs_y_session)\n",
    "        patients_epochs[patients_id.index(data_loader.patient_id)].append(epochs_all)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'patient id: {data_loader.patient_id}')\n",
    "            print(f'session id: {data_loader.session_id}')\n",
    "            print(f'number of epochs: {epochs_X_session.shape[0]}')\n",
    "            print(f'number of channels: {epochs_X_session.shape[1]}')\n",
    "            print(f'number of time samples: {epochs_X_session.shape[2]}')\n",
    "\n",
    "    if return_epochs:\n",
    "        return patients_data, patients_labels, patients_id, sessions_id, patients_epochs\n",
    "    else: \n",
    "        return patients_data, patients_labels, patients_id, sessions_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_patients, y_patients, patients_id, sessions_id, epochs = preproc(\n",
    "    FILES_PATHS[:NB_SESSIONS], verbose=False, return_epochs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization CSP patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from mne.preprocessing import Xdawn\n",
    "\n",
    "n_components = 5\n",
    "n_patient_train = 0\n",
    "n_session_train = 1\n",
    "\n",
    "csp = mne.decoding.CSP(n_components=n_components)\n",
    "sc  = StandardScaler()\n",
    "pip = Pipeline([('csp', csp), ('sc2', sc)])\n",
    "\n",
    "pip.fit(X_patients[n_patient_train][n_session_train],\n",
    "        y_patients[n_patient_train][n_session_train]); \n",
    "# On fit avec le 1er patient et la première session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_csp(csp_data, labels):\n",
    "    # Définir la taille de chaque vignette\n",
    "    figsize_per_subplot = 3  # Choisissez la taille appropriée en pouces\n",
    "\n",
    "    # Calculer la taille totale de la figure en fonction du nombre de sous-plots\n",
    "    figsize = (n_components * figsize_per_subplot, n_components * figsize_per_subplot)\n",
    "\n",
    "    # Créer une figure avec une grille spécifique\n",
    "    fig, axs = plt.subplots(n_components-1, \n",
    "                            n_components-1, \n",
    "                            figsize=figsize, gridspec_kw={'wspace': 0.4, 'hspace': 0.4})\n",
    "\n",
    "    for i in range(n_components):\n",
    "        for j in range(i+1, n_components):\n",
    "            # Sélectionner la première composante CSP\n",
    "            csp_component_y = csp_data[:, i]\n",
    "\n",
    "            # Sélectionner la dernière composante CSP\n",
    "            csp_component_x = csp_data[:, j]\n",
    "\n",
    "            # Faire un graphe avec la première composante en ordonnées et la dernière en abscisses\n",
    "            ax = axs[i, j-1]  # Utilisation de la grille spécifique\n",
    "            ax.scatter(csp_component_y, csp_component_x, c=labels, cmap='viridis')\n",
    "            ax.set_xlim(-2, 2)\n",
    "            ax.set_ylim(-2, 2)\n",
    "            ax.set_xlabel(f'Composante CSP n°{j}')\n",
    "            ax.set_ylabel(f'Composante CSP n°{i}')\n",
    "\n",
    "    # Ajuster automatiquement la disposition des sous-plots pour éviter le chevauchement\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtenir les données transformées par la CSP\n",
    "n_patient = 0\n",
    "n_session = 0\n",
    "csp_data = pip.transform(X_patients[n_patient][n_session])\n",
    "labels = y_patients[n_patient][n_session]\n",
    "plot_csp(csp_data, labels)\n",
    "\n",
    "# n_patient = 3\n",
    "# n_session = 0\n",
    "# csp_data = csp.transform(X_patients[n_patient][n_session])\n",
    "# labels = y_patients[n_patient][n_session]\n",
    "# plot_csp(csp_data, labels)\n",
    "\n",
    "# n_patient = 4\n",
    "# n_session = 0\n",
    "# csp_data = csp.transform(X_patients[n_patient][n_session])\n",
    "# labels = y_patients[n_patient][n_session]\n",
    "# plot_csp(csp_data, labels)\n",
    "\n",
    "# couple_patient_session = [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0)]\n",
    "\n",
    "# for n_patient, n_session in couple_patient_session:\n",
    "#     csp_data = csp.transform(X_patients[n_patient][n_session])\n",
    "#     labels = y_patients[n_patient][n_session]\n",
    "#     plot_csp(csp_data, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization xDAWN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.viz import plot_epochs_image\n",
    "\n",
    "motor_electrods_d = [\n",
    "    'FC2', 'FC4', 'FC6', \n",
    "     'C2',  'C4',  'C6',\n",
    "    'CP2', 'CP4', 'CP6',]\n",
    "motor_electrods_g = [\n",
    "    'FC1', 'FC3', 'FC5',\n",
    "     'C1',  'C3',  'C5',\n",
    "    'CP1', 'CP3', 'CP5']\n",
    "\n",
    "id_patient = 4\n",
    "id_session = 0\n",
    "\n",
    "motor_electrods = motor_electrods_d + motor_electrods_g\n",
    "picked_channels = [e for e in motor_electrods if e in epochs[id_patient][id_session].ch_names]\n",
    "\n",
    "plot_epochs_image(\n",
    "    epochs=epochs[id_patient][id_session], \n",
    "    picks=picked_channels,\n",
    "    vmin=-20,\n",
    "    vmax=+20,\n",
    "    colorbar=True, \n",
    "    show=True,\n",
    "    cmap='interactive',);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = mne.preprocessing.Xdawn(n_components=2)\n",
    "xd.fit(epochs[id_patient][id_session])\n",
    "\n",
    "epochs_denoised = xd.apply(epochs[id_patient][:])\n",
    "\n",
    "plot_epochs_image(\n",
    "    epochs=epochs_denoised['0'], \n",
    "    picks=picked_channels,\n",
    "    vmin=-2,\n",
    "    vmax=+2,\n",
    "    colorbar=True, \n",
    "    show=True,\n",
    "    cmap='interactive',);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline evaluation strategies\n",
    "\n",
    "We will define 3 test strategies to verify the performance of our pipeline.\n",
    "\n",
    "1. **[A]** test on the same patients and same sessions as view in the training. Of course, the epochs will not be the same, but the classifier will have already seen data from each patient and session considered.\n",
    "\n",
    "2. **[B]** test on the same patients, but using different sessions. This is to check is the classifier is able to perform well on already seen patients but with data that can be quite different (improvement between sessions).\n",
    "\n",
    "3. **[C]** test on completely new patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    balanced_accuracy_score,)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_SPLITS = 4\n",
    "\n",
    "def split_folds_test_A(patients_data):\n",
    "    \"\"\"\n",
    "    Split train/test on same patients, same sessions, just shuffling the epochs.\n",
    "\n",
    "    split_folds     = [split_patient_1, split_patient_2, ...]\n",
    "    split_patient_i = [split_session_1, split_session_2, ...]\n",
    "    split_session_i = [train_idx, test_idx]\n",
    "    \"\"\"\n",
    "    split_folds = []\n",
    "    index = 0\n",
    "    for patient in range(len(patients_data)):\n",
    "        split_folds.append([])\n",
    "        for session in range(len(patients_data[patient])):\n",
    "            rs = RANDOM_STATE + index\n",
    "            cv = KFold(n_splits=NB_SPLITS, shuffle=True, random_state=rs)\n",
    "            split_folds[patient].append(cv.split(patients_data[patient][session]))\n",
    "            index += 1 \n",
    "    return split_folds\n",
    "\n",
    "def split_folds_test_B(patients_data):\n",
    "    \"\"\"\n",
    "    Split train/test on same patients, different sessions.\n",
    "\n",
    "    split_folds     = [split_patient_1, split_patient_2, ...]\n",
    "    split_patient_i = [all_sessions_1, all_sessions_2, ...]\n",
    "    all_sessions_i  = [train_idx, test_idx]\n",
    "    \"\"\"\n",
    "    split_folds = []\n",
    "    index = 0\n",
    "    for patient in range(len(patients_data)):\n",
    "        split_folds.append([])\n",
    "        for i_split in range(NB_SPLITS):\n",
    "            # If only 1 session for the patient, go train\n",
    "            if len(patients_data[patient]) < 2:\n",
    "                test_idx  = []\n",
    "                train_idx = [0]\n",
    "            else:\n",
    "                shuffle_i = np.arange(len(patients_data[patient]))\n",
    "                rng = np.random.RandomState(RANDOM_STATE + index)\n",
    "                rng.shuffle(shuffle_i)\n",
    "                test_idx  = list(shuffle_i[-1:])\n",
    "                train_idx = list(shuffle_i[:-1])\n",
    "            split_folds[-1].append([train_idx, test_idx])\n",
    "            index += 1\n",
    "    return split_folds\n",
    "\n",
    "def split_folds_test_C(patients_data):\n",
    "    \"\"\"\n",
    "    Split train/test on different patients, different sessions.\n",
    "\n",
    "    split_folds     = [split_fold_1, split_fold_2, ...]\n",
    "    split_fold_i    = [train_idx, test_idx]\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=NB_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    return cv.split(patients_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(split_folds_test_A(X_patients)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(split_folds_test_B(X_patients)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(split_folds_test_C(X_patients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluations_test_A(clf, data_X, data_y, verbose=False):\n",
    "    split_folds = split_folds_test_A(data_X)\n",
    "    scores = []\n",
    "\n",
    "    for i in range(NB_SPLITS):\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        X_test  = []\n",
    "        y_test  = []\n",
    "        for patient in range(len(data_X)):\n",
    "            for session in range(len(data_X[patient])):\n",
    "                train_idx, test_idx = next(split_folds[patient][session])\n",
    "                X_train.append(data_X[patient][session][train_idx])\n",
    "                y_train.append(data_y[patient][session][train_idx])\n",
    "                X_test.append(data_X[patient][session][test_idx])\n",
    "                y_test.append(data_y[patient][session][test_idx])\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test  = np.concatenate(X_test, axis=0)\n",
    "        y_test  = np.concatenate(y_test, axis=0)\n",
    "\n",
    "        print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        scores.append(balanced_accuracy_score(y_test, y_pred))\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Split {i} - balanced accuracy: {scores[-1]}')\n",
    "    if verbose:\n",
    "        print(f'Mean balanced accuracy: {np.mean(scores)}')\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluations_test_B(clf, data_X, data_y, verbose=False):\n",
    "    split_folds = split_folds_test_B(data_X)\n",
    "    scores = []\n",
    "\n",
    "    for i in range(NB_SPLITS):\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        X_test  = []\n",
    "        y_test  = []\n",
    "        for patient in range(len(data_X)):\n",
    "            for session in split_folds[patient][i][0]:\n",
    "                X_train.append(data_X[patient][session])\n",
    "                y_train.append(data_y[patient][session])\n",
    "            for session in split_folds[patient][i][1]:\n",
    "                X_test.append(data_X[patient][session])\n",
    "                y_test.append(data_y[patient][session])\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test  = np.concatenate(X_test, axis=0)\n",
    "        y_test  = np.concatenate(y_test, axis=0)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        scores.append(balanced_accuracy_score(y_test, y_pred))\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Split {i} - balanced accuracy: {scores[-1]}')\n",
    "    if verbose:\n",
    "        print(f'Mean balanced accuracy: {np.mean(scores)}')\n",
    "\n",
    "    return scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluations_test_C(clf, data_X, data_y, verbose=False):\n",
    "    split_folds = split_folds_test_C(data_X)\n",
    "    scores = []\n",
    "    index = 0\n",
    "\n",
    "    for train_patient_i, test_patient_i in split_folds:\n",
    "        train_X = [data_X[i][j] for i in train_patient_i for j in range(len(data_X[i]))]\n",
    "        train_y = [data_y[i][j] for i in train_patient_i for j in range(len(data_y[i]))]\n",
    "        test_X  = [data_X[i][j] for i in test_patient_i for j in range(len(data_X[i]))]\n",
    "        test_y  = [data_y[i][j] for i in test_patient_i for j in range(len(data_y[i]))]\n",
    "\n",
    "        train_X = np.concatenate(train_X, axis=0)\n",
    "        train_y = np.concatenate(train_y, axis=0)\n",
    "        test_X  = np.concatenate(test_X, axis=0)\n",
    "        test_y  = np.concatenate(test_y, axis=0)\n",
    "\n",
    "        # Shuffle\n",
    "        train_idx = np.arange(train_X.shape[0])\n",
    "        rng = np.random.RandomState(RANDOM_STATE + index)\n",
    "        rng.shuffle(train_idx)\n",
    "        train_X = train_X[train_idx]\n",
    "        train_y = train_y[train_idx]\n",
    "        \n",
    "        test_idx = np.arange(test_X.shape[0])\n",
    "        rng = np.random.RandomState(RANDOM_STATE + index)\n",
    "        rng.shuffle(test_idx)\n",
    "        test_X = test_X[test_idx]\n",
    "        test_y = test_y[test_idx]\n",
    "        index += 1\n",
    "\n",
    "        clf.fit(train_X, train_y)\n",
    "\n",
    "        y_pred = clf.predict(test_X)\n",
    "        scores.append(balanced_accuracy_score(test_y, y_pred))\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Split {index} - balanced accuracy: {scores[-1]}')\n",
    "    if verbose:\n",
    "        print(f'Mean balanced accuracy: {np.mean(scores)}')\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test pipeline: [CSP+classif] \n",
    "\n",
    "Train on one session, test on the other session of the same patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_electrods_d = [\n",
    "    'FC2', 'FC4', 'FC6', \n",
    "     'C2',  'C4',  'C6',\n",
    "    'CP2', 'CP4', 'CP6',]\n",
    "motor_electrods_g = [\n",
    "    'FC1', 'FC3', 'FC5',\n",
    "     'C1',  'C3',  'C5',\n",
    "    'CP1', 'CP3', 'CP5']\n",
    "\n",
    "id_patient = 5\n",
    "id_session = 0\n",
    "\n",
    "motor_electrods = motor_electrods_d + motor_electrods_g\n",
    "picked_channels = [e for e in motor_electrods if e in epochs[id_patient][id_session].ch_names]\n",
    "\n",
    "\n",
    "epochs[id_patient][1].plot_image(picks=picked_channels, vmin=-20, vmax=20);\n",
    "epochs[id_patient][0].plot_image(picks=picked_channels, vmin=-20, vmax=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from mne.decoding import CSP\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# i_patient = 1\n",
    "# i_session = 0\n",
    "\n",
    "for i_patient in range(len(X_patients)):\n",
    "    print(f'Patient {i_patient}')\n",
    "    if len(X_patients[i_patient]) < 2:\n",
    "        print(f\"  Abort, not enough sessions\")\n",
    "        continue\n",
    "    for i_session in range(len(X_patients[i_patient])):\n",
    "        X_patient = X_patients[i_patient][i_session]\n",
    "        y_patient = y_patients[i_patient][i_session]\n",
    "        X_test = X_patients[i_patient][(1+i_session)%2]\n",
    "        y_test = y_patients[i_patient][(1+i_session)%2]\n",
    "\n",
    "        csp = CSP(n_components=4)\n",
    "        sc  = StandardScaler()\n",
    "        svc = SVC(kernel='linear', C=.001)\n",
    "        # rf  = RandomForestClassifier()\n",
    "        pip = Pipeline([('CSP', csp), ('sc', sc), ('SVC', svc)])\n",
    "\n",
    "        pip.fit(X_patient, y_patient)\n",
    "        score = pip.score(X_test, y_test)\n",
    "\n",
    "        print(f'  Session {i_session} on train - score: {score} on session {(1+i_session)%2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline 1**: CSP + classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from mne.decoding import CSP\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define pipeline\n",
    "csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "clf = SVC(C=1, kernel='linear', class_weight='balanced', random_state=RANDOM_STATE)\n",
    "\n",
    "# Define pipeline\n",
    "pip = Pipeline([('CSP', csp), ('SVC', clf)])\n",
    "\n",
    "# Ensemble classifier\n",
    "ens = BaggingClassifier(base_estimator=pip, n_estimators=10, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test A scores\n",
    "scores_A = evaluations_test_A(pip, X_patients, y_patients, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test B scores\n",
    "scores_B = evaluations_test_B(clf, X_patients, y_patients, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test C scores\n",
    "scores_C = evaluations_test_C(clf, X_patients, y_patients, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Test on multiple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# Define classifier\n",
    "classifier = [\n",
    "    ('SVC', SVC(kernel='linear')),\n",
    "    ('RF', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "    ('LR', LogisticRegression(random_state=RANDOM_STATE)),\n",
    "    ('LDA', LinearDiscriminantAnalysis())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "accuracies = []\n",
    "for clf in classifier:\n",
    "    print(f\"Testing classifier: {clf[0]} ...\")\n",
    "    csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "    pip = Pipeline([('CSP', csp), clf])\n",
    "\n",
    "    # Test A scores\n",
    "    scores_A = evaluations_test_A(pip, X_patients, y_patients)\n",
    "    accuracies.append(scores_A)\n",
    "\n",
    "df = pd.DataFrame({'clf': ['SVC', 'RF', 'LR', 'LDA'], 'acc': np.mean(accuracies, axis=1)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "accuracies = []\n",
    "for clf in classifier:\n",
    "    print(f\"Testing classifier: {clf[0]} ...\")\n",
    "    csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "    pip = Pipeline([('CSP', csp), clf])\n",
    "\n",
    "    # Test B scores\n",
    "    scores_B = evaluations_test_B(pip, X_patients, y_patients)\n",
    "    accuracies.append(scores_B)\n",
    "\n",
    "df = pd.DataFrame({'clf': ['SVC', 'RF', 'LR', 'LDA'], 'acc': np.mean(accuracies, axis=1)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "accuracies = []\n",
    "for clf in classifier:\n",
    "    print(f\"Testing classifier: {clf[0]} ...\")\n",
    "    csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "    pip = Pipeline([('CSP', csp), clf])\n",
    "\n",
    "    # Test C scores\n",
    "    scores_C = evaluations_test_C(pip, X_patients, y_patients)\n",
    "    accuracies.append(scores_C)\n",
    "\n",
    "df = pd.DataFrame({'clf': ['SVC', 'RF', 'LR', 'LDA'], 'acc': np.mean(accuracies, axis=1)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline 2**: xDAWN + Vectorizer + Std_Scaler + Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MyVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.features_shape_ = X.shape[1:]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.reshape(len(X), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyriemann.estimation import XdawnCovariances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "xd = XdawnCovariances()\n",
    "sc = StandardScaler()\n",
    "vc = MyVectorizer()\n",
    "\n",
    "classifier = [\n",
    "    ('SVC', SVC(kernel='linear')),\n",
    "    ('RF', RandomForestClassifier(random_state=RANDOM_STATE)),\n",
    "    ('LR', LogisticRegression(random_state=RANDOM_STATE, penalty='l1', solver='liblinear')),\n",
    "    ('LDA', LinearDiscriminantAnalysis())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "accuracies = []\n",
    "for clf in classifier:\n",
    "    print(f\"Testing classifier: {clf[0]} ...\")\n",
    "    pip = Pipeline([\n",
    "        ('XD', xd),\n",
    "        ('VC', vc),\n",
    "        ('SC', sc), clf])\n",
    "\n",
    "    # Test A scores\n",
    "    scores_A = evaluations_test_A(pip, X_patients, y_patients)\n",
    "    accuracies.append(scores_A)\n",
    "\n",
    "df = pd.DataFrame({'clf': ['SVC', 'RF', 'LR', 'LDA'], 'acc': np.mean(accuracies, axis=1)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "accuracies = []\n",
    "for clf in classifier:\n",
    "    print(f\"Testing classifier: {clf[0]} ...\")\n",
    "    pip = Pipeline([\n",
    "        ('XD', xd),\n",
    "        ('VC', vc),\n",
    "        ('SC', sc),\n",
    "        clf])\n",
    "\n",
    "    # Test A scores\n",
    "    scores_B = evaluations_test_B(pip, X_patients, y_patients)\n",
    "    accuracies.append(scores_B)\n",
    "\n",
    "df = pd.DataFrame({'clf': ['SVC', 'RF', 'LR', 'LDA'], 'acc': np.mean(accuracies, axis=1)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "accuracies = []\n",
    "for clf in classifier:\n",
    "    print(f\"Testing classifier: {clf[0]} ...\")\n",
    "    pip = Pipeline([\n",
    "        ('XD', xd),\n",
    "        ('VC', vc),\n",
    "        ('SC', sc),\n",
    "        clf])\n",
    "\n",
    "    # Test A scores\n",
    "    scores_C = evaluations_test_C(pip, X_patients, y_patients)\n",
    "    accuracies.append(scores_C)\n",
    "\n",
    "df = pd.DataFrame({'clf': ['SVC', 'RF', 'LR', 'LDA'], 'acc': np.mean(accuracies, axis=1)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline 3**: Riemannian geom + tangent space + scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = Covariances(estimator='oas')\n",
    "ts  = TangentSpace()\n",
    "sc  = StandardScaler()\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "pip = Pipeline([('Cov', cov), ('TS', ts), ('SC', sc), ('CLF', clf)])\n",
    "\n",
    "scoreA = evaluations_test_A(pip, X_patients, y_patients, verbose=True)\n",
    "scoreB = evaluations_test_B(pip, X_patients, y_patients, verbose=True)\n",
    "scoreC = evaluations_test_C(pip, X_patients, y_patients, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = Covariances(estimator='oas')\n",
    "ts  = TangentSpace()\n",
    "sc  = StandardScaler()\n",
    "clf = SVC()\n",
    "\n",
    "pip = Pipeline([('Cov', cov), ('TS', ts), ('SC', sc), ('CLF', clf)])\n",
    "\n",
    "scoreA = evaluations_test_A(pip, X_patients, y_patients, verbose=True)\n",
    "scoreB = evaluations_test_B(pip, X_patients, y_patients, verbose=True)\n",
    "scoreC = evaluations_test_C(pip, X_patients, y_patients, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline 4**: CSP + Classifier (voting method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from mne.decoding import CSP\n",
    "\n",
    "N_COMPONENTS = 5\n",
    "N_SPLITS = 5\n",
    "\n",
    "\n",
    "class VotingClassifier():\n",
    "    def __init__(self) -> None:\n",
    "        self.clfs = []\n",
    "        self.y_true = None\n",
    "\n",
    "    def fit_clf_session(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the inner classifiers on sessions data. \n",
    "        NB: one session is a list of epochs.\n",
    "\n",
    "        X: list of epochs\n",
    "        y: list of labels\n",
    "        \"\"\"\n",
    "        csp = CSP(n_components=N_COMPONENTS, reg=None, log=True, norm_trace=False)\n",
    "        sc = StandardScaler()\n",
    "        svc = SVC(kernel='linear', C=0.1)\n",
    "        clf = Pipeline([('CSP', csp), ('SC', sc), ('SVC', svc)])\n",
    "\n",
    "        clf.fit(X, y)\n",
    "        return clf\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the outer classifier on sessions data.\n",
    "\n",
    "        X: list of sessions, each session is a list of epochs\n",
    "        y: list of sessions, each session is a list of labels\n",
    "        \"\"\"\n",
    "        clfs = []\n",
    "        for session in range(len(X)):\n",
    "            clfs.append(self.fit_clf_session(X[session], \n",
    "                                             y[session]))\n",
    "        self.clfs = clfs\n",
    "\n",
    "    def predict(self, X, return_probas=False):\n",
    "        \"\"\"\n",
    "        Predicts labels of data.\n",
    "\n",
    "        X: list of epochs\n",
    "        \"\"\"\n",
    "        X = [epoch for session in X for epoch in session]\n",
    "        X = np.array(X)\n",
    "\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for i in range(len(self.clfs)):\n",
    "            clf = self.clfs[i]\n",
    "            y_pred = clf.predict(X)\n",
    "            preds += y_pred\n",
    "\n",
    "\n",
    "            # Affichage des résultats pour le classifieur n°i\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.scatter(np.arange(len(y_pred)), y_pred, c=self.y_true, cmap='viridis')\n",
    "            plt.title(f'Result for classifier n°{i}')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        preds = preds / len(self.clfs)\n",
    "\n",
    "        if return_probas:\n",
    "            return (preds > 0.5).astype(int), preds\n",
    "        else:\n",
    "            return (preds > 0.5).astype(int)\n",
    "    \n",
    "    def score(self, X, y, return_probas=False):\n",
    "        \"\"\"\n",
    "        Scores the classifier.\n",
    "        \n",
    "        X: list of sessions, each session is a list of epochs\n",
    "        y: list of sessions, each session is a list of labels\n",
    "        \"\"\"\n",
    "        y_flat = np.array([label for session in y for label in session])\n",
    "        y_pred = self.predict(X, return_probas=False)\n",
    "        return balanced_accuracy_score(y_flat, y_pred)\n",
    "\n",
    "\n",
    "def cv_test_alt(data_X, data_y, verbose=False, display=False):\n",
    "    \"\"\"\n",
    "    data_X: list of sessions, each session is a list of epochs\n",
    "    data_y: list of sessions, each session is a list of labels\n",
    "    \"\"\"\n",
    "    # Define cross validation fold\n",
    "    cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in cv.split(data_X):\n",
    "        # Split data\n",
    "        X_train = [data_X[i] for i in train_idx]\n",
    "        y_train = [data_y[i] for i in train_idx]\n",
    "        X_test  = [data_X[i] for i in test_idx]\n",
    "        y_test  = [data_y[i] for i in test_idx]\n",
    "\n",
    "        # Define pipelines + voting classifier\n",
    "        pipeline = VotingClassifier()\n",
    "\n",
    "        # Fit\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Score\n",
    "        score = pipeline.score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "        if display:\n",
    "\n",
    "            y_true = np.array([label for session in y_test for label in session])\n",
    "            pipeline.y_true = y_true\n",
    "            y_pred, probas = pipeline.predict(X_test, return_probas=True)\n",
    "\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.scatter(np.arange(len(probas)), probas, c=y_true, cmap='viridis')\n",
    "            plt.show()\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Balanced accuracy: {score}')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Mean balanced accuracy: {np.mean(scores)}')\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sessions = [session for patient in X_patients for session in patient]\n",
    "y_sessions = [session for patient in y_patients for session in patient]\n",
    "\n",
    "cv_test_alt(X_sessions, y_sessions, verbose=True, display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEGstroke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
